% Copyright 2022 Jeffrey Kegler
% This document is licensed under
% a Creative Commons Attribution-NoDerivs 3.0 United States License.
\documentclass[12pt]{amsart}
\usepackage{amssymb}
\usepackage{xspace}
\usepackage{url}
\usepackage{placeins}
\usepackage{needspace}
\usepackage{float}
% \usepackage[obeyDraft]{todonotes}
\usepackage{todonotes}

\usepackage{pgf}
\usepackage{tikz}
\usetikzlibrary{arrows,automata}
\usetikzlibrary{positioning}

\tikzset{
    state/.style={
           rectangle,
           rounded corners,
           draw=black, thick,
           minimum height=2em,
           inner sep=2pt,
           text centered,
           },
}

% We deal with underfull vbox's by eyeball, or not at all.
\raggedbottom

\newcommand{\dfn}[1]{{\bf #1}}
\newcommand{\inference}[2]{\genfrac{}{}{1pt}{}{#1}{#2}}
\newcommand{\mymathop}[1]{\mathop{\texttt{#1}}}
\newcommand{\V}[1]{\ensuremath{\texttt{\mbox{#1}}}}
\newcommand{\type}[1]{\ensuremath{\texttt{#1}}}

\newcommand{\isWellDef}{\mathop{\downarrow}}
\newcommand{\isIllDef}{\mathop{\uparrow}}
\newcommand{\wellDefRel}{\mathrel{\downarrow}}
\newcommand{\illDefRel}{\mathrel{\uparrow}}
\newcommand{\wellDefB}[2]{#1\wellDefRel#2}
\newcommand{\illDefB}[2]{#1\illDefRel#2}

\newcommand{\qeq}{\simeq}
\newcommand{\nqeq}{\not\simeq}
\newcommand{\unicorn}{\ensuremath{\bot}\xspace}
\newcommand{\TRUE}{\ensuremath{\mathbb T}\xspace}
\newcommand{\FALSE}{\ensuremath{\mathbb F}\xspace}
\newcommand{\tuple}[1]{\ensuremath{\left\langle #1 \right\rangle}}

\newcommand{\seq}[1]{\ensuremath{\left[ #1 \right]}}
\newcommand{\seqB}[2]{\seq{#1 \mcoloncolon #2}}

\newcommand{\typed}[2]{\ensuremath{#1\mcolon#2}}
\newcommand{\Vtyped}[2]{\ensuremath{\V{#1}\mcolon#2}}
\newcommand{\VTtyped}[2]{\ensuremath{\V{#1}\mcolon\type{#2}}}

\newcommand{\mname}[1]{\mbox{\sf #1}}
% [O]perator [A]pplication
\newcommand{\Oname}[1]{\mname{#1}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\OA}[2]{\ensuremath{%
    \mathop{\Oname{#1}}\mathopen{}\left(#2\right)\mathclose{}%
}}
\newcommand{\VOA}[2]{\OA{#1}{\V{#2}}}
% [F]unction [A]pplication, replaces \myfn
\newcommand{\smallrawfn}[2]{\ensuremath{\mathop{#1}(#2)}}
% The \mathopen{} and \mathclose{} eliminate unwanted extra space
\newcommand{\rawfn}[2]{\ensuremath{\mathop{#1}\mathopen{}\left(#2\right)\mathclose{}}}
\newcommand{\Fname}[1]{\V{#1}}
\newcommand{\FA}[2]{\ensuremath{\rawfn{\mathop{\Fname{#1}}}{#2}}}
\newcommand{\VFA}[2]{\FA{#1}{\V{#2}}}

% List operators
\newcommand{\Hd}[1]{\OA{hd}{#1}}
% With the list operators,
% is looks better if we don't increase paren size as we nest
\newcommand{\Tl}[1]{\Oname{tl}({#1})}
\newcommand{\HTl}[1]{\Hd{\Tl{#1}}}
\newcommand{\TTl}[1]{\Tl{\Tl{#1}}}
\newcommand{\HTTl}[1]{\Hd{\Tl{\Tl{#1}}}}
\newcommand{\TTTl}[1]{\Tl{\TTl{#1}}}

\newcommand{\defined}{\underset{\text{def}}{\equiv}}
\newcommand{\naturals}{\ensuremath{\mathbb{N}}\xspace}
\newcommand{\cat}{\mathbin{\scalebox{2}{.}}}

\newcommand{\size}[1]{\ensuremath{\left | {#1} \right |}}
\newcommand{\Vsize}[1]{\ensuremath{\size{\V{#1}}}}

\newcommand{\de}{\mathrel{::=}}
\newcommand{\derives}{\mathrel{\Rightarrow}}
\newcommand{\destar}
    {\mathrel{\mbox{$\stackrel{\!{\ast}}{\Rightarrow\!}$}}}
\newcommand{\deplus}
    {\mathrel{\mbox{$\stackrel{\!{+}}{\Rightarrow\!}$}}}
\newcommand{\mydot}{\raisebox{.05em}{$\,\bullet\,$}}

\newcommand{\lastix}[1]{\ensuremath{\##1}}
\newcommand{\Vlastix}[1]{\ensuremath{\lastix{\V{#1}}}}
\newcommand{\VlastElement}[1]{\Velement{#1}{\Vlastix{#1}}}
\newcommand{\element}[2]{\ensuremath{\mathop{#1}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\Velement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[#2\right]\mathclose{}}}
\newcommand{\VVelement}[2]{\ensuremath{\mathop{\V{#1}}
    \mathopen{}\left[ \V{#2}
    \right]\mathclose{} }}

\newcommand{\mcolon}{\mathrel:}
\newcommand{\mcoloncolon}{\mathrel{\vcenter{\hbox{$::$}}}}

\newcommand{\suchthat}{\mcoloncolon}
\newcommand{\quantify}[3]{% quantifier, binding, predicate
    \ensuremath{\mathrel{#1}#2 \; \suchthat \; #3}%
}
\newcommand{\rIota}{\mathop{\scalebox{1.5}{\rotatebox[origin=c]{180}{$\iota$}}}}
\newcommand{\iotaQOp}{\mathop{\rIota}}
\newcommand{\iotaQ}[2]{\quantify{\iotaQOp}{#1}{#2}}

\newcommand{\epsQOp}{\mathop{\varepsilon}}
\newcommand{\epsQ}[2]{\quantify{\epsQOp}{#1}{#2}}

\newcommand{\lambdaQOp}{\mathop{\lambda}}
\newcommand{\lambdaQ}[2]{\quantify{\lambdaQOp}{#1}{#2}}

% collection builder variant, or descriptor
\newcommand{\setQvOp}{\mathop{\mathsection}}
\newcommand{\setQv}[2]{\quantify{\setQOp}{#1}{#2}}
\newcommand{\setB}[2]{\lbrace #1 \; \suchthat \; #2 \rbrace}

\newcommand{\existUniqQOp}{\mathop{\exists{!}}}
\newcommand{\existUniqQ}[2]{\quantify{\existUniqQOp}{#1}{#2}}
% extra {} in existQOP and univQOp to raise to baseline
\newcommand{\existQOp}{\mathop{\exists{}}}
\newcommand{\existQ}[2]{\quantify{\existQOp}{#1}{#2}}
\newcommand{\nexistQOp}{\mathop{\nexists{}}}
\newcommand{\nexistQ}[2]{\quantify{\nexistQOp}{#1}{#2}}
% extra {} in existQOP and univQOp to raise to baseline
\newcommand{\univQOp}{\mathop{\forall{}\,}}
\newcommand{\univQ}[2]{\quantify{\univQOp}{#1}{#2}}

% [f]u[n]ction [M]ap operator
\newcommand{\fnMap}{\rightharpoonup}
% [t]otal [f]u[n]ction [M]ap operator
\newcommand{\tfnMap}{\rightarrow}

% [t]otal [f]u[n]ction [M]ultidomain [M]ap operator
% \hbox shifts from math to text vertical alignment, which is what we want
\newcommand{\tfnMMap}{\ensuremath{
    \mathrel{\raise.75pt\hbox{\ensuremath{
          \raise1.19pt\hbox{\scalebox{.60}{$\ni$}} \mkern-12mu \rightarrow}}}
    }}

\newcommand{\nat}[1]{\ensuremath{#1_\naturals}}
\newcommand{\Vnat}[1]{\nat{\V{#1}}}

\newcommand{\ah}[1]{#1_{\type{AH}}}
\newcommand{\Vah}[1]{\ensuremath{\V{#1}_{\type{AH}}}}
\newcommand{\dr}[1]{#1_{\type{DR}}}
\newcommand{\Vdr}[1]{\ensuremath{\V{#1}_{\type{DR}}}}
\newcommand{\orig}[1]{#1_{\type{ORIG}}}
\newcommand{\Vorig}[1]{\ensuremath{\V{#1}_{\type{ORIG}}}}
\newcommand{\setOf}[1]{{{#1}^{\mbox{\normalsize $\ast$}}}}
\newcommand{\Vrule}[1]{\ensuremath{\V{#1}_{\type{RULE}}}}
\newcommand{\Vruleset}[1]{\ensuremath{\V{#1}_{\setOf{\type{RULE}}}}}
\newcommand{\Veim}[1]{\ensuremath{\V{#1}_{\type{EIM}}}}
\newcommand{\es}[1]{\ensuremath{#1_{\type{ES}}}}
\newcommand{\Ves}[1]{\ensuremath{\V{#1}_{\type{ES}}}}
\newcommand{\Vstr}[1]{\ensuremath{\V{#1}_{\type{STR}}}}
\newcommand{\sym}[1]{#1_{\type{SYM}}}
\newcommand{\Vsym}[1]{\ensuremath{\V{#1}_{\type{SYM}}}}
\newcommand{\Vsymset}[1]{\ensuremath{\V{#1}_{\setOf{\type{SYM}}}}}
\newcommand{\Eloc}[1]{\ensuremath{{#1}_{\type{LOC}}}}
\newcommand{\Vloc}[1]{\Eloc{\V{#1}}}

\newcommand{\Cfa}{\V{fa}}
\newcommand{\Cg}{\V{g}}
\newcommand{\Cw}{\V{w}}
\newcommand{\Crules}{\V{rules}}
\newcommand{\Nulling}[1]{\mymathop{Nulling}(#1)}
\newcommand{\Nullable}[1]{\mymathop{Nullable}(#1)}
\newcommand{\GOTO}[2]{\mymathop{GOTO}(#1, #2)}
\newcommand{\Next}[1]{\mymathop{Next}(#1)}
\newcommand{\Predict}[1]{\mymathop{Predict}(#1)}
\newcommand{\Postdot}[1]{\mymathop{Postdot}(#1)}
\newcommand{\Penult}[1]{\mymathop{Penult}(#1)}
\newcommand{\Pos}[1]{\mymathop{Pos}(#1)}
\newcommand{\Rule}[1]{\mymathop{Rule}(#1)}
\newcommand{\LHS}[1]{\mymathop{LHS}(#1)}
\newcommand{\RHS}[1]{\mymathop{RHS}(#1)}
\newcommand{\Rules}[1]{\ensuremath{\mymathop{Rules}(#1)}}
\newcommand{\Vocab}[1]{\ensuremath{\mymathop{Vocab}(#1)}}
\newcommand{\Terminals}[1]{\ensuremath{\mymathop{Terminals}(#1)}}
\newcommand{\Accept}[1]{\ensuremath{\mymathop{Accept}(#1)}}
\newcommand{\DR}[1]{\mymathop{DR}(#1)}
\newcommand{\Transition}[1]{\mymathop{Transition}(#1)}
\newcommand{\Origin}[1]{\mymathop{Origin}(#1)}
\newcommand{\Current}[1]{\mymathop{Current}(#1)}
\newcommand{\myL}[1]{\mymathop{L}(#1)}

\newcommand\Etable[1]{\ensuremath{\mymathop{table}[#1]}}
\newcommand\bigEtable[1]{\ensuremath{\mymathop{table}\bigl[#1\bigr]}}
\newcommand\Vtable[1]{\Etable{\V{#1}}}

\newsavebox{\myBox}
\newlength{\myHt}
\newlength{\myDp}
\newlength{\myWd}
\newcommand{\padBoxC}[3]{%
    \savebox{\myBox}{\ignorespaces#3}%
    \usebox{\myBox}%
    \myHt=\ht\myBox%
    \myDp=\dp\myBox%
    \vrule height\dimexpr\myHt+#1 depth\dimexpr\myDp+#2 width0pt%
}
\newcommand{\padBox}[2]{\padBoxC{#1}{#1}{#2}}

\newcommand{\cellboxWidth}[1]{4.5in}

% non-optional-argument version of cellbox
\newcommand{\cellboxB}[2]{%
    \parbox{#1}{\ignorespaces#2}%
}

% Padded cellbox
\newcommand{\cellbox}[2][\cellboxWidth]{%
    \cellboxB{#1}{%
        \padBoxC{4pt}{0pt}{\vphantom{(}}%
        {\ignorespaces#2}%
        \padBoxC{0pt}{4pt}{\vphantom{(}}%
    }%
}

\begin{document}

\date{\today}

\title{Marpa and nullable symbols}

\author{Jeffrey Kegler}
\thanks{%
Copyright \copyright\ 2023 Jeffrey Kegler.  Version 1.
}
\thanks{%
This document is licensed under
a Creative Commons Attribution-NoDerivs 3.0 United States License.
}

\begin{abstract}
Marpa is an improved Earley parser.
Earley-based parsers have had issues handling
nullable symbols.
Initially, we dealt with nullable symbols by following the approach
in Aycock and Horspool's 2002 paper~\cite{AH2002}.
This paper reports our experience with ~\cite{AH2002},
and the approach to handling nullables that we settled on
in reaction to that experience.
\end{abstract}

\maketitle

\section{Overview}

The Marpa recognizer~\cite{Marpa2023} was intended
to make the best results on Earley's algorithm
in the academic literature available as
a practical general parser.
Accordingly, when Marpa was first released in 2011,
it included the improved handling of nullable symbols
in Aycock and Horspool's 2002 paper~\cite{AH2002}.
The measures proposed in
that we adopted from \cite{AH2002}
were the following:
\begin{itemize}
\item Eliminating proper nullable with a semantics preserving
grammar rewrite.
A ``proper nullable'' symbol is a nulling symbol
that is not nulling.
\item
Collecting dotted rules into states of a
finite automata, which we will call
an Aycok-Horspool Finite Automata (AHFA)\footnote{%
  In \cite{AH2002} an AHFA is called a ``split LR(0) $\epsilon$-DFA''.
},
and modifying the Earley parse engine to use
AHFA states.
\end{itemize}

Section \ref{sec:preliminaries}
deals with notation and other conventions.
Sections \ref{sec:earley}
and \ref{sec:earley-ops}
describe the Earley algorithm.
Sections \ref{sec:aycock-horspool-ideal-solution}
and \ref{sec:aycock-horspool-finite-automata}
describe the Aycock-Horspool version of Earley's
algorithm~\cite{AH2002}.
Sections \ref{sec:pitfall-of-counting-earley-items}
and \ref{sec:ahfa-states-are-not-disjoint}
raise some theoretical issues about \cite{AH2002}.
Section \ref{sec:problems-with-the-ahfa}
discusses the practical issues we encountered in
implementing~\cite{AH2002}.
In 2014 we extensively modified Marpa's usage of
the ideas from \cite{AH2002}.
Section
\ref{sec:marpa-s-approach-to-dealing-with-nullable-symbols}
describes the new approach
we took to dealing with nullables.
Section \ref{sec:conclusion} summarizes our conclusions.

\section{Preliminaries}
\label{sec:preliminaries}

Readers should be familiar with Marpa~\cite{Marpa2023},
with Aycock and Horspool's 2002 paper~\cite{AH2002},
and with standard grammar notation.

We use the type system
of Farmer~2012~\cite{Farmer2012},
without needing most of its apparatus.\footnote{%
Types in \cite{Farmer2012} are
collections of classes (``superclasses'').
But in this paper, every explicitly stated type will be a ZF set.
}
Let \V{exp} be an expression,
and let \type{T} be its type.
Type may be indicated in a ``wide'' notation:
\[ \VTtyped{exp}{T} \]
More often, this paper will use the ``narrow''
notation,
where a subscripts indicates type:
\[ \V{exp}_\type{T} \]

A noteworthy feature we adopt from Farmer~2012~\cite{Farmer2012} is his notion of ill-definedness.
For example, the value of partial functions may be ill-defined for
some arguments in their domain.
Farmer's handling of ill-definedness is the traditional one,
and was well-entrenched,
but he was first to describe and formalize it.\footnote{%
See Farmer 2004~\cite{Farmer2004}.
Note that Farmer refers to ill-defined values as ``undefined''.
We found this problematic.
For example, a partial function may not have a value for
every argument in its domain.
Saying that the value of the partial function for these arguments
is defined as ``undefined'' is confusing.
In this paper we say that all the values of partial functions
are defined, but that some may not have values in the codomain,
and are therefore ill-defined.
}

We write \TRUE for ``true''; \FALSE for ``false'';
and $\unicorn$ for ill-defined.
A value is \dfn{well-defined} iff it is not ill-defined.
We write $\V{x}\isWellDef$ to say that \V{x} is well-defined,
and we write $\V{x}\isIllDef$ to say that \V{x} is ill-defined.

Traditionally, and in this paper,
any formula with an ill-defined operand is false.
This means that an equality both of whose operands are ill-defined
is false, so that $\unicorn \neq \unicorn$.
For cases where this is inconvenient,
we introduce a new relation, $\simeq$, such that
$$
  \V{a} \simeq \V{b} \defined \V{a} = \V{b} \lor (\V{a} \isIllDef \land \V{b} \isIllDef ).
$$

We often abbreviate ``if and only if'' to ``iff''.
We define the natural numbers, \naturals, to include zero.
$\V{f} \circ \V{g}$, pronounced ``\V{f} after \V{g}'',
indicates the composition of the functions \V{f} and \V{g},
so that
$\rawfn{(\V{f}\circ\V{g})}{\V{x}} = \VFA{f}{\VFA{g}{\V{x}}}$.

We write tuples using angle brackets: $ \tuple{\V{a},\V{b},\V{c}} $.
The head of a tuple \V{S} can be written \Hd{S}.
The tail of a tuple \V{S} can be written \Tl{S}.
For example, where
\[
    \V{S} = \tuple{ 42, 1729, 42 },
\]
then \V{S} is a 3-tuple, or triple,
\begin{gather*}
\Hd{S}=42, \\
\Tl{S}=\tuple{1729,42}, \\
\HTl{S}=1729, \\
\TTl{S}=42, \text{ and} \\
\TTTl{S}\isIllDef.
\end{gather*}

We define the natural numbers, \naturals, to include zero.
$\V{D} \fnMap \V{C}$ is the set of partial functions from domain \V{D} to codomain \V{C}.
$\V{D} \tfnMap \V{C}$ is the set of total functions from domain \V{D} to codomain \V{C}.
It follows that $\naturals \tfnMap \V{C}$ is the set of
infinite sequences of terms from the set \V{C};
and that $42 \tfnMap \V{C}$ is the set of
sequences of length 42 of terms from the set \V{C}.
We say that
$$ \V{domSet} \tfnMMap \V{C} \defined
   \bigl\{ \V{fn} \bigm|
       \left( \exists \; \V{D} \in \V{domSet} \; \middle| \; \V{fn} \in \V{D} \tfnMap \V{C}
       \right) \bigr\}
$$
so that $\naturals \tfnMMap \V{C}$
is the set of finite sequences of terms from the set \V{C}.

We write \Vsize{\V{seq}} for the cardinality, or length,
of the sequence \V{seq}.
\VVelement{seq}{i} is the \V{i}'th term of the sequence \V{seq},
and is well-defined when
$0 \le \V{i} < \Vsize{seq}$.
We often specify a sequence by giving its terms inside
brackets.
For example,
$$\seq{\, \V{a}, \; 42, \; \seq{} \, }$$
is the sequence of length
3 whose terms are, in order, \V{a}, the number 42, and the empty sequence.

The last index of the sequence \V{seq} is \Vlastix{seq},
so that \VlastElement{seq} is the last term of \V{seq}.
\Vlastix{seq} is ill-defined for the empty sequence,
that is, if $\Vsize{seq} = 0$.
If \V{seq} is not the empty sequence, then
$\Vsize{seq} = \Vlastix{seq} + 1$.

Let \type{SYM} be the type for symbols, which
will will treat as opaque,
except that
\( \emptyset \notin \type{SYM} \).
The string type, \type{STR}, is the set of all finite sequences of
symbols:
\[ \type{STR} = \naturals \tfnMMap \type{SYM}. \]
The empty string is \( \emptyset \), but we let
\( \emptyset = \epsilon \)
and we usually write the empty string as \( \epsilon \).
We note also that \(\epsilon\) can usually be used as
the reserved non-symbol: \( \epsilon \notin \type{SYM} \).

A rule (type \type{RULE}) is a duple:
\[ \type{RULE} = \type{SYM} \times \type{STR}. \]
\[ \begin{gathered}
    \LHS{\VTtyped{r}{RULE}} = \Hd{\V{r}}. \\
    \RHS{\VTtyped{r}{RULE}} = \Tl{\V{r}}.
\end{gathered} \]
We usually write a rule \V{r} in
the form $\tuple{\Vsym{lhs} \de \Vstr{rhs}}$,
where $\V{lhs} = \LHS{\V{r}}$
and $\V{rhs} = \RHS{\V{r}}$.
\Vsym{lhs} is referred to as the left hand side (LHS)
of \Vrule{r}.
\Vstr{rhs} is referred to as the right hand side (RHS)
of \Vrule{r}.

A grammar is a 4-tuple, type \type{G},
\[
      \type{G} = \setOf{\type{SYM}} \times \setOf{\type{SYM}} \times  \type{RULE} \times \type{SYM},
\]
where
\[ \begin{gathered}
      \Vocab{\VTtyped{g}{G}} \defined \Hd{\V{g}}, \\
      \Terminals{\VTtyped{g}{G}} \defined \HTl{\V{g}}, \\
      \Rules{\VTtyped{g}{G}} \defined \HTTl{\V{g}}, \text{ and}\\
      \Accept{\VTtyped{g}{G}} \defined \TTTl{\V{g}}, \\
\end{gathered} \]
and where, if \V{g} is a grammar, then
\[ \begin{gathered}
      \Accept{\V{g}} \in \Vocab{\V{g}}, \\
      \Terminals{\V{g}} \subset \Vocab{\V{g}} \setminus \Terminals{\V{g}}, \\
      \V{r} \in \Rules{\V{g}} \implies \LHS{\V{r}} \in \Vocab{\V{g}} \setminus \Terminals{\V{g}},
	\text{ and }\\
      \V{r} \in \Rules{\V{g}} \implies \RHS{\V{r}} \in \setOf{\Vocab{\V{g}}}.
\end{gathered} \]

The rules of a grammar imply the traditional rewriting system,
in which
\begin{itemize}
\item $\Vstr{x} \derives \Vstr{y}$
states that \V{x} derives \V{y} in exactly one step;
\item $\Vstr{x} \deplus \Vstr{y}$
states that \V{x} derives \V{y} in one or more steps;
and
\item $\Vstr{x} \destar \Vstr{y}$
states that \V{x} derives \V{y} in zero or more steps.
\end{itemize}
We call these rewrites \dfn{derivation steps}.
A sequence of one or more derivation steps,
in which the left hand side of all but the first
is the right hand side of its predecessor,
is a \dfn{derivation}.
We say that the symbol \V{x} \dfn{induces}
the string of length 1 whose only term is that symbol,
that is, the string \seq{\Vsym{x}}.
Pedantically, the terms of derivations,
and the arguments of concatenations like
$\Vstr{s1} \cat \Vstr{s2}$
must be strings.
But in concatenations and derivation steps we often
write the symbol to represent the string it induces so
that
$$ \Vstr{a} \cat \Vsym{b} \cat \Vstr{c}
= \Vstr{a} \cat \seq{\Vsym{b}} \cat \Vstr{c}.
$$

A \dfn{sentence} of \V{g} is a string of terminals derivable
from \V{g}'s accept symbol.
The language of a grammar \V{g}, $\myL{\V{g}}$,
is its set of sentences:
\[ \begin{gathered}
  \myL{\V{g}} = \setB{ \VTtyped{sentence}{STR} }{ \\
  \V{sentence} \in \setOf{\Terminals{\V{g}}} \land  \Accept{\V{g}} \destar \V{sentence} }
\end{gathered} \]

We say that a string \V{x} is \dfn{nullable},
$\Nullable{\Vstr{x}}$, iff the empty string can be
derived from it:
$$\Nullable{\Vstr{x}} \defined \V{x} \destar \epsilon.$$
We say that a string \V{x} is \dfn{nulling},
$\Nulling{\Vstr{x}}$,
iff it always eventually derives the null
string:
\[
\Nulling{\Vstr{s}} \defined
  \forall \; \Vstr{y} \;\mid\; \V{x} \destar \V{y} \implies \V{y} \destar \epsilon.
\]
We say that symbols are nulling or nullable based on the string they
induce:
\begin{gather*}
\Nullable{\Vsym{x}} \defined \Nullable{\seq{\V{x}}}. \\
\Nulling{\Vsym{x}} \defined \Nulling{\seq{\V{x}}}.
\end{gather*}
A string or symbol is
\begin{itemize}
\item \dfn{non-nullable} iff it is not nullable;
\item a \dfn{proper nullable} iff it is nullable,
but not nulling; and
\item \dfn{non-nulling} iff it is not nulling.
\end{itemize}

We assume grammars are ``augmented''.
Specifically, for any grammar \V{g},
there is an accept rule,
\begin{equation}
\label{eq:accept-rule}
  \Vrule{accept} = \tuple{ \Vsym{accept} \de \Vsym{start} },
\end{equation}
where \( \Vsym{accept} = \Accept{\V{g}} \)
and \( \Vsym{start} \in \Vocab{\V{g}}; \)
\begin{equation*}
\begin{gathered}
\forall \; \V{x} \in \Rules{\V{g}} \mid \\
\nexists \; \Vstr{pre}, \; \Vstr{post} \; \mid \\
\V{pre} \cat \Vsym{accept} \cat \V{post} = \RHS{\Vrule{x}}; \\
\end{gathered}
\end{equation*}
and the accept rule is unique,
\[ \Vsym{accept} = \LHS{\V{x}} \implies \Vrule{accept} = \V{x}. \]

Let the input to
the parse be $\typed{\Cw}{\type{STR}^\ast}$.
Locations in the input will be of type \type{LOC}.
When we state our complexity results later,
they will often be in terms of $\V{n}$,
where $\V{n} = \Vsize{w}$.
\typed{\VVelement{w}{i}}{\type{SYM}} is character \Vloc{i}
of the input,
and is well-defined when
$0 \le \Vloc{i} < \Vsize{w}$.

\section{Earley's algorithm}
\label{sec:earley}

A dotted rule (type \type{DR}) is a duple of rule and position
within the rule.
\begin{gather*}
\Rule{\Vdr{dr}} \defined \Hd{\V{dr}}. \\
\Pos{\Vdr{dr}} \defined \Tl{\V{dr}}.
\end{gather*}
More precisely,
\[ \begin{gathered}
\type{DR} \defined \setB{ \, \Vtyped{dr}{\type{RULE} \times \naturals}}{
    0 \le \Pos{\V{dr}} \le \size{\Rule{\V{dr}}} \, }.
\end{gathered} \]

The position of a dotted rule indicates the extent to which
the rule has been recognized,
and is represented with a large raised dot,
so that if
\begin{equation*}
\tuple{ \Vsym{A} \de \Vsym{X} \cat \Vsym{Y} \cat \Vsym{Z}}
\end{equation*}
is a rule,
\begin{equation*}
\tuple{ \Vsym{A} \de \V{X} \cat \V{Y} \mydot \V{Z}}
\end{equation*}
is the dotted rule with the dot at
$\V{pos} = 2$,
between \Vsym{Y} and \Vsym{Z}.

Every rule concept, when applied to a dotted rule,
is applied to the rule of the dotted rule.
The following are examples:
\[
\begin{gathered}
  \LHS{\Vdr{x}} \defined \LHS{\Rule{\V{x}}}. \\
  \RHS{\Vdr{x}} \defined \RHS{\Rule{\V{x}}}.
\end{gathered}
\]

We also make the following definitions:
\[
  \Postdot{\Vdr{x}} \defined
  \begin{cases}
  \Vsym{next}, \text{ if } \existQ{
      \Vstr{pre}, \; \Vstr{post}, \; \Vsym{A}}{ \\
      \qquad \V{x} = \tuple{ \V{A} \de \V{pre} \mydot \V{next} \cat \V{post}}} \\
  \unicorn, \text{ otherwise.}
  \end{cases} \\
\]

\[
  \Next{\Vdr{x}} \defined
  \begin{cases}
    \VTtyped{\tuple{ \Vsym{A} \de \Vstr{pre} \cat \Vsym{next} \mydot \Vstr{post}}}{DR}, \\
      \qquad \text{if } \V{x} = \tuple{ \V{A} \de \V{pre} \mydot \V{next} \cat \V{post}} \\
    \unicorn, \text{ otherwise.}
  \end{cases} \\
\]

\[
  \Penult{\Vdr{x}} \defined
  \begin{cases}
  \Vsym{next}, \text{ if } \existQ{
      \Vstr{pre}, \; \Vstr{post}, \; \Vsym{A}}{ \\
      \qquad \V{x} = \tuple{ \V{A} \de \V{pre} \mydot \V{next} \cat \V{post}} \\
      \qquad \land \; \Nulling{\V{post}} \land \; \neg \Nulling{\V{next}}} \\
  \unicorn, \text{ otherwise}
  \end{cases}
\]
\todo{Delete Penult?}

A \dfn{penult} is a dotted rule \Vdr{d} such that $\Penult{\V{d}} \isWellDef$.
We note that $\Penult{\Vdr{x}}$
is never a nullable symbol.
The \dfn{initial dotted rule} is
\begin{equation}
\label{eq:initial-dr}
\Vdr{initial} = \tuple{\Vsym{accept} \de \mydot \Vsym{start} },
\end{equation}
where \Vsym{accept} and \Vsym{start} are as in
the accept rule,
\eqref{eq:accept-rule}
on page \pageref{eq:accept-rule}.
A \dfn{predicted dotted rule} is a dotted rule,
other than the initial dotted rule,
with a dot position of zero,
for example,
\begin{equation*}
\Vdr{predicted} = \tuple{\Vsym{A} \de \mydot \Vstr{alpha} }.
\end{equation*}
A \dfn{confirmed dotted rule}
is the initial dotted rule,
or a dotted rule
with a dot position greater than zero.
A \dfn{completed dotted rule} is a dotted rule with its dot
position after the end of its RHS,
for example,
\begin{equation*}
\Vdr{completed} = \tuple{\Vsym{A} \de \Vstr{alpha} \mydot }.
\end{equation*}
Predicted, confirmed and completed dotted rules
are also called, respectively,
\dfn{predictions}, \dfn{confirmations} and \dfn{completions}.

An Earley item (type \type{EIM}) is a triple
of dotted rule, origin, and current location.\footnote{%
  This definition of \type{EIM} departs from the tradition,
  according to which the current location is not an element
  of the tuples which define \type{EIM}s.
  Instead \type{EIM}s
  are grouped into sets
  that share the same current location.
  These sets of co-located \type{EIMS}s are called
  ``Earley sets''.
  Membership in an Earley sets becomes a ``property''
  of each \type{EIM}.
  In set theory, by the Axion of Extensionality,
  sets with the same membership (or extension)
  are equivalent,
  and non-extensional properties do not exist.
  Since \type{EIM}s at different current locations
  are conceptually distinct for most purposes,
  the omission of current location from the \type{EIM}
  definition is awkward,
  and in this paper we honor the traditional definition in the breach.
}
\begin{gather*}
     \DR{\Veim{x}} = \Hd{\V{x}}. \\
     \Origin{\Veim{x}} = \HTl{\V{x}}. \\
     \Current{\Veim{x}} = \TTl{\V{x}}.
\end{gather*}
$\DR{\Veim{x}}$ is the dotted rule.
The origin of \Veim{x},
$$\typed{\Origin{\Veim{x}}}{\type{LOC}},$$
is the location in the input where recognition of $\Rule{\DR{\Veim{x}}}$
started.
The current location of \Veim{x},
$$\typed{\Current{\Veim{x}}}{\type{LOC}},$$
is the location in the input of the dot in $\DR{\Veim{x}}.$
$\Current{\Veim{x}}$ is also called the \dfn{dot location}
of \Veim{x}.
For convenience, the type \type{ORIG} will be a synonym
for \type{LOC}, indicating that the variable designates
the origin entry of an Earley item.

We find it convenient to apply dotted rule concepts to
\type{EIM}'s,
so that the concept applied to the \type{EIM} is
the concept applied to the dotted rule of the \type{EIM}.
The following are examples:
\begin{gather*}
\LHS{\Veim{x}} \defined \LHS{\DR{\V{dr}}}. \\
\RHS{\Veim{x}} \defined \RHS{\DR{\V{dr}}}. \\
\Pos{\Veim{x}} \defined \Pos{\DR{\V{dr}}}. \\
\Postdot{\Veim{x}} \defined \Postdot{\DR{\V{dr}}}. \\
\Penult{\Veim{x}} \defined \Penult{\DR{\V{dr}}}. \\
\Next{\Veim{x}} \defined \Next{\DR{\V{dr}}}. \\
\Rule{\Veim{x}} \defined \Rule{\DR{\V{dr}}}.
\end{gather*}

An Earley parser builds a table of Earley sets,
\begin{equation*}
\Vtable{i},
\quad \text{where} \quad
0 \le \Vloc{i} \le \size{\Cw}.
\end{equation*}
Earley sets are of type \type{ES}.
Earley sets are often named by their location:
That is,
the bijection between \Ves{i} and \Vloc{i}
allows locations to be treated as the ``names'' of Earley sets.
We often write \Ves{i} to mean the Earley set at \Vloc{i},
and \Vloc{x} to mean the location of Earley set \Ves{x}.
The type designator \type{ES} is often omitted to avoid clutter,
especially in cases where the Earley set is not
named by location.
Occasionally the naming location is a expression,
so that
$$ \es{(\Origin{\Veim{x}})} $$
is the Earley set at the origin of the \type{EIM} \Veim{x}.
If \es{\V{working}} is an Earley set,
$\size{\es{\V{working}}}$ is the number of Earley items.

Recalling the accept rule,
\eqref{eq:accept-rule}
on page \pageref{eq:accept-rule},
we say that
the input \Cw{} is accepted if and only if
\begin{equation*}
\tuple{\tuple{\Vsym{accept} \de \Vsym{start} \mydot}, 0} \in \bigEtable{\Vsize{\Cw}}.
\end{equation*}

\section{Operations of the Earley algorithm}
\label{sec:earley-ops}

\textbf{Initialization}:
\begin{equation}
\label{eq:initial}
\inference{
   \TRUE
}{
    \begin{array}{c}
        \VTtyped{ \tuple{ \Vdr{initial}, 0, 0 }}{EIM} \\
    \end{array}
}
\end{equation}
Here \Vdr{initial} is
from \eqref{eq:initial-dr} on \pageref{eq:initial-dr}.
Earley {\bf initialization} only takes
place in Earley set 0,
and always adds exactly one \type{EIM}.

\vspace{1ex}
\textbf{Scanning}:
\begin{equation}
\label{eq:scan}
\inference{
    \begin{array}{c}
	\Postdot{\Veim{mainstem}} = \Cw\bigl[ \Vloc{current} \bigr]
    \end{array}
}{
    \begin{array}{c}
	\VTtyped{ \tuple{ \Next{\Veim{mainstem}}, \Origin{\V{mainstem}},
	    \V{current}+1
	}}{EIM}
    \end{array}
}
\end{equation}

\vspace{1ex}
\textbf{Reduction}:
\begin{equation}
\label{eq:reduction}
\inference{
    \begin{array}{c}
      \Current{\Veim{mainstem}} = \Origin{\Veim{tributary}} \\
      \Postdot{\Veim{mainstem}} = \LHS{\Veim{tributary}}
    \end{array}
}{
    \begin{array}{c}
	\left< \begin{gathered}
	  \Next{\Veim{mainstem}}, \Origin{\V{mainstem}}, \\
	       \Current{\V{tributary}}
	\end{gathered} \right> : \type{EIM}
    \end{array}
}
\end{equation}

\vspace{1ex}
\textbf{Prediction}:
\begin{equation}
\label{eq:prediction}
\inference{
    \begin{array}{c}
	\tuple{ \Vsym{lhs} \de \Vstr{rhs} } \in \Rules{\V{g}} \\
	\Postdot{\V{mainstem}} = \Vsym{lhs}
    \end{array}
}{
  \begin{array}{c}
      \left< \begin{gathered}
      \tuple{ \Vsym{lhs} \de \mydot \Vstr{rhs} }, \Current{\V{mainsteam}}, \\
	  \Current{\V{mainsteam}}
      \end{gathered} \right> : \type{EIM}
  \end{array}
}
\end{equation}

In traditional implementations,
the operations are applied
to create Earley sets,
in order from 0 to \Vsize{w}.
Duplicate \type{EIM}s are not added.
In typical implementations,
scanning is run
ahead of the other operations
in the sense that, while the other operations are adding
items to the Earley set at \Vloc{i},
scanned items are added to \es{(\V{i}+1)}.

Traditionally, each Earley set is implementated as a list.
The result of a prediction operation,
\eqref{eq:prediction} on page \pageref{eq:prediction},
may be the \V{mainstem} of a reduction operation,
\eqref{eq:reduction} on page \pageref{eq:reduction}.
That reduction operation may, in turn,
produce new predictions.
Typically, an implementation dealt with this
by making repeated passes
through the Earley set,
terminating when no more \type{EIM}s could be added.

\section{The Aycock-Horspool ``ideal'' solution}
\label{sec:aycock-horspool-ideal-solution}

The need to make multiple passes over each Earley set
was long seen as a problem.
Aycock and Horspool experimented with a suggestion
from Jay Earley that required
a dynamically-updated data structure,
but found this solution unsatisfactory~\cite[p. 621]{AH2002}.
Instead, they found a way to revise the prediction step
itself that eliminated the problem.

To present Aycock and Horspool's revised prediction step,
we first rewrite the original prediction operation
\eqref{eq:prediction} on page \pageref{eq:prediction},
in the form of a function,
\[ \Vtyped{Opred}{\type{EIM} \tfnMap \setOf{\type{EIM}}}, \]
such that
\begin{equation}
\label{eq:original-earley-prediction}
\begin{gathered}
\VFA{Opred}{\Veim{x}} =
\left\lbrace
    \Veim{res} \; \middle| \;
    \begin{gathered}
     \LHS{\V{res}} = \Postdot{\V{eim}} \\
     \land \, \Rule{\V{res}} \in \Rules{\V{g}} \\
     \land \, \Pos{\V{res}} = 0 \\
     \land \, \Current{\V{res}} = \Current{\Veim{x}} \\
     \land \, \Origin{\V{res}} = \Current{\Veim{x}}
     \end{gathered}
\right\rbrace . \end{gathered}
\end{equation}

Let
\( \Vtyped{AHpred}{\type{EIM} \tfnMap \setOf{\type{EIM}}}. \)
be
the transitive closure of \Fname{Opred}.
More precisely,
\begin{equation}
\label{eq:ah-earley-prediction}
\VFA{AHpred}{\VTtyped{x}{EIM}} = \Fname{predAll} \circ \Fname{Opred},
\end{equation}
where
\begin{equation*}
\begin{gathered}
\VFA{predAll}{\Vtyped{eimSet}{\setOf{\type{EIM}}}} =  \\
  \setB{\VTtyped{eim}{EIM}}{ \existQ{\Vtyped{i}{\naturals}}{
     \V{eim} \in \VFA{predN}{\V{i}, \V{eimSet}} } }, \\[1ex]
\VFA{predN}{0, \Vtyped{eimSet}{\setOf{\type{EIM}}}} = \V{eimSet}, \text{ and} \\[1ex]
\VFA{predN}{(\Vtyped{n}{\naturals})+1, \Vtyped{eimSet}{\setOf{\type{EIM}}}} = \\
  \setB{ \VTtyped{eim}{EIM} }{ \existQ{\VTtyped{eim2}{EIM}}{ \\
    \V{eim} \in \VFA{Opred}{\V{eim2}} \\
    \land \, \V{eim2} \in \VFA{predN}{\V{n},\V{eimset}}}}.
\end{gathered}
\end{equation*}

Aycock and Horspool proved \cite[pp. 621-622]{AH2002} that,
by replacing the Earley prediction operation
(equation \ref{eq:original-earley-prediction}
on page \pageref{eq:original-earley-prediction}),
with its transitive closure
(equation \ref{eq:ah-earley-prediction}
on page \pageref{eq:ah-earley-prediction}),
they had created an algorithm that successful dealt with
nullable symbols,
required only one pass,
``retain[ed] the elegance of Earley's algorithm'',
and did not require a new, dynamically-updated data structure.

\section{The Aycock-Horspool finite automata}
\label{sec:aycock-horspool-finite-automata}

Aycock and Horspool called the algorithm of
the previous section ``ideal'',
but the quote marks are theirs
\cite[p. 621]{AH2002}.
This seems to reflect a perception on their part that,
while they had an idea
that allowed an Earley-based parse engine
to handle nullable symbols gracefully,
a practical implemention would demand some refinements.

The ``ideal'' solution of~\cite{AH2002},
if naively implemented,
required every prediction operation to compute a transitive closure.
But, during the parse, this transitive closure was a constant ---
it depended only on the grammar and the postdot symbol of the argument EIM.
Clearly, precomputation could eliminate most or all of the runtime
cost of their new prediction operation.

For their precomputation, Aycock and Horspool~\cite{AH2002}
invented a new semi-deterministic finite automata,
which \cite{AH2002} calls a ``split LR(0) $\epsilon$-DFA''.
We recall that, in this paper,
we are calling their ``split LR(0) $\epsilon$-DFA'',
an Aycock-Horspool Finite Automata (AHFA).

The AHFA is based
on a few observations.
\begin{itemize}
\item
In practice, Earley items sharing the same origin,
but having different dotted rules,
often appear together in the same Earley set.
\item
There is in the literature a method
for associating groups of dotted rules that often appear together
when parsing.
This method is the LR(0) DFA used in the much-studied
LALR and LR parsers.
\item
The LR(0) items that are the components of LR(0)
states are, exactly, dotted rules.
\item
By taking into account symbols that derive the
null string, the LR(0) DFA could be turned into an
LR(0) $\epsilon$-DFA,
which would be even more effective
at grouping dotted rules that often occur together
into a single DFA state.
\end{itemize}

Aycock and Horspool realized that,
by changing Earley items to track AHFA states
instead of individual dotted rules,
the size of Earley sets could be reduced,
and conjectured that this would
make Earley's algorithm faster in practice.

An AHFA state (type \type{AH}) is, in effect, a shorthand
for groups of dotted rules that occur together frequently.
\[ \type{AH} = \setOf{\type{DR}}. \]
We recall that
the traditional Earley items (\type{EIM}'s)
are duples,
\[ \tuple{\VTtyped{x}{DR}, \VTtyped{i}{LOC}}, \]
where \Vdr{x} is a dotted rule.
An Aycock-Horspool Earley item is a duple
\[ \tuple{\VTtyped{x}{AH}, \VTtyped{i}{LOC}}. \]

AHFA's are not fully deterministic.
\cite{AH2002} also defines
a partial transition function for
pairs of AHFA state and symbol,
\begin{equation*}
\Vtyped{GOTO}{ \type{AH} \times (\epsilon \cup \Vocab{\V{g}}) \fnMap \type{AH} }.
\end{equation*}
A transition on the reserved non-symbol,
for example
\( \GOTO{\Vah{from)}}{\epsilon}, \)
is a \dfn{null transition}.
If \Vah{predicted} is the result of a null transition,
it is called a \dfn{predicted} AHFA state.
If an AHFA state is not a \dfn{predicted} AHFA state,
it is called a \dfn{confirmed} AHFA state.
The initial AHFA state is a confirmed AHFA state.\footnote{%
In~\cite{AH2002} confirmed states are called ``kernel states'',
and predicted states are called ``non-kernel states''.
}

\section{The pitfall of counting Earley items}
\label{sec:pitfall-of-counting-earley-items}

In the Earley parsing literature speed results
are reported based on the count of Earley items -- the fewer Earley items, it is
assumed, the faster the parser. This totally ignores evaluation, never
mind tracing or run-time features like events.

This massive over-simplification worked, because Jay Earley and Joop
Leo only went for results in Landau notation, and these are "big" enough
that even a huge over-simplification is not a problem. This is not the
case when we are talking about a constant factor.

\section{AHFA states are not disjoint}
\label{sec:ahfa-states-are-not-disjoint}

The states of an AHFA
are not a partition of the dotted
rules --
a single dotted rule can occur
in more than one AHFA state.

\begin{figure}
\hrule
\caption{\label{fig:ahfa}
    AHFA example
}
\vspace{.5ex}
\hrule
\vspace{.5\baselineskip}
\begin{tikzpicture}

 \node[state,
     anchor=center,
 ] (C0)
 {\begin{tabular}{ll}
   C0&$\V{S}^\prime \de \mydot \V{S}$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=C0,
     node distance=4.5cm] (C8)
 {\begin{tabular}{ll}
   C8&$\V{S}^\prime \de \V{S} \mydot$
   \end{tabular}};

 \node[state,
     anchor=center,
     below of=C0,
     node distance=15ex] (P1)
 {\begin{tabular}{ll}
   P1&$\V{S} \de \mydot \V{A}\cat\V{B}$ \\
   &$\V{A} \de \mydot \V{x}\cat\V{a}$ \\
   &$\V{A} \de \mydot \V{B}$ \\
   &$\V{B} \de \mydot \V{x}\cat\V{b}$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=P1,
     node distance=4.5cm] (C2)
 {\begin{tabular}{ll}
   C2&$\V{A} \de \V{x}\mydot\V{a}$ \\
   &$\V{B} \de \V{x}\mydot\V{b}$
   \end{tabular}};

 \node[state,
     anchor=center,
     below of=P1,
     node distance=15ex] (C3)
 {\begin{tabular}{ll}
   C3&$\V{S} \de \V{A}\mydot\V{B}$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=C3,
     node distance=4.5cm] (C4)
 {\begin{tabular}{ll}
   C4&$\V{S} \de \V{A}\cat\V{B}\mydot$
   \end{tabular}};

 \node[state,
     anchor=center,
     below of=C3,
     node distance=12ex] (P2)
 {\begin{tabular}{ll}
   P2&$\V{B} \de \mydot \V{x}\cat\V{b}$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=P2,
     node distance=4.5cm] (C5)
 {\begin{tabular}{ll}
   C5&$\V{B} \de \V{x}\mydot\V{b}$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=C4,
     node distance=4.5cm] (C6)
 {\begin{tabular}{ll}
   C6&$\V{B} \de \V{x}\cat\V{b}\mydot$
   \end{tabular}};

 \node[state,
     anchor=center,
     right of=C2,
     node distance=4.5cm] (C7)
 {\begin{tabular}{ll}
   C7&$\V{A} \de \V{x}\cat\V{a}\mydot$
   \end{tabular}};

  \path[->] (C0) edge node[anchor=left,right] {$\epsilon$} (P1) ;
  \path[->] (C0) edge node[anchor=south,above] {$\V{S}$} (C8) ;
  \path[->] (P1) edge node[anchor=south,above] {$\V{x}$} (C2) ;
  \path[->] (P1) edge node[anchor=left,right] {$\V{A}$} (C3) ;
  \path[->] (C3) edge node[anchor=left,right] {$\epsilon$} (P2) ;
  \path[->] (C3) edge node[anchor=south,above] {$\V{B}$} (C4) ;
  \path[->] (P2) edge node[anchor=south,above] {$\V{x}$} (C5) ;
  \path[->] (C2) edge node[anchor=south,above] {$\V{a}$} (C7) ;
  \path[->] (C2) edge node[anchor=south,above] {$\V{b}$} (C6) ;
  \path[->] (C5) edge node[anchor=south,above] {$\V{b}$} (C6) ;
\end{tikzpicture}
\vspace{.5\baselineskip}
\hrule
\end{figure}

The relationship between AHFA items and Earley items is not
many-to-one, but many-to-many, greatly complicating the translation. IIRC
correctly this is not in their paper. I ran into it when
implementing.

We were able to measure the AHFA states for practical grammars, only to
find that their actual effect was quite small. The AHFA states did not
produce measurable improvements, and were serious obstacles to several
run-time features I was contemplating.

The problem of translating back and forth from LR rule sets to the grammar
is the most important factor that limited the success of LR parsing. When a Yacc parse
fails it is often very hard for the grammar writer to figure out why.

AHFA states are not a partitioning of the dotted rules.  One dotted
rule can fall into more than one AHFA state.
Aycock and Horspool saw this as an optimization, in
that it allows dotted rules, if they often occur together, to be grouped
together aggressively. But it opens up the possibility that, in cases
where the Earley algorithm had disposed of a dotted rule once and for all, Marpa
might have to deal with it multiple times.

Jay Earley~\cite{Earley1970},
in his proofs,
could rely on the fact
that for each attempt
to add a duplicate,
the causation must be different --
that the confluences causing the attempt
will differ in either their mainstem
or their tributary.
Multiple confluences for an Earley item
would mean multiple derivations
for the sentential form that it represents.
That in turn would mean that
the grammar is ambiguous,
contrary to assumption.

With AHFA states, there is complication.
A dotted rule can occur in more than one AHFA
state.
Because of that,
it is possible that two of Marpa's
operations to add an \type{EIM}
will represent identical Earley confluences,
and therefore will be
consistent with an unambiguous grammar.
Dealing with this produced complications both for our
theoretical results.
We~\cite{Marpa2023}
and Aycock and Horpsool~\cite{AH2002}
were able to achieve the same results in
Landau-notation terms as Earley~\cite{Earley1970}
but getting there was more complicated.
Similarly, it made
the Marpa implementation much more complex.

\section{Problems with the AHFA}
\label{sec:problems-with-the-ahfa}

\subsection{Difficulty in debugging and maintenance}

\todo{Tables of Earley items use AH-states, not dotted rules}

\todo{Dotted rules are a natural way of tracking a parse,
collected them in AH-states requires a lookup which is a
nuisance when tracing and debugging}

\subsection{Difficulty in adding new features}

\subsection{Increased code complexity}

\subsection{Increased theoretical complexity}

\subsection{No noticeable performance improvement}

\todo{Given the forgoing, we would have been willing
to give up some performance}

\section{Marpa's approach to dealing with nullable symbols}
\label{sec:marpa-s-approach-to-dealing-with-nullable-symbols}

\section{Conclusion}
\label{sec:conclusion}

\bibliographystyle{plain}

\begin{thebibliography}{10}

\raggedright

\bibitem{AU1972}
Alfred H.~Aho and Jeffrey D.~Ullman.
\newblock The Theory of Parsing, Translation, and Computing
\newblock Prentice-Hall, Englewood Cliff, N.J., 1972.

\bibitem{AH2002}
John~Aycock and R.~Nigel~Horspool.
\newblock Practical Earley Parsing
\newblock {\em The Computer Journal},
    Vol. 45, No. 6, 2002, pp. 620-630.

\bibitem{Earley1970}
J.~Earley.
\newblock An efficient context-free parsing algorithm.
\newblock {\em Communications of the Association for Computing Machinery},
  13(2):94--102, 1970.

\bibitem{Farmer2004}
William~M.~Farmer.
\newblock Formalizing undefinedness arising in calculus.
\newblock In D.~Basin and M.~Rusinowitch, editors, {\em Automated
  Reasoning---IJCAR 2004}, volume 3097 of {\em Lecture Notes in Computer
  Science}, pages 475--489. Springer-Verlag, 2004.

\bibitem{Farmer2012}
William~M.~Farmer.
\newblock Chiron: A Set Theory with Types, Undefinedness, Quotation, and Evaluation.
\newblock SQRL Report No. 38, McMaster University, 2007 (revised 2012)
\newblock arXiv:1305.6206

\bibitem{GJ2008}
Dirk~Grune and Ceriel~J.H Jacobs.
\newblock {\em Parsing Techniques: A Practical Guide}.
\newblock Springer, Amsterdam, 2008.

\bibitem{Marpa2023}
Jeffrey~Kegler.
\newblock Marpa, A practical general parser: the recognizer.
\\ % fix underfull HBOX
\newblock arXiv:1910.08129.
\newblock \url{https://arxiv.org/abs/1910.08129}

\bibitem{Marpa-R2}
Jeffrey~Kegler, 2013: Marpa-R2.
\newblock \url{http://search.cpan.org/dist/Marpa-R2/}.

\bibitem{Leo1991}
J.~M. I.~M. Leo.
\newblock A general context-free parsing algorithm running in linear time on
  every {LR($k$)} grammar without using lookahead.
\newblock {\em Theoretical Computer Science}, 82:165--176, 1991.

\end{thebibliography}

\Needspace{30\baselineskip}

\appendix

\section{ Statistics on AHFA states. }

\subsection{Confirmed states.}

\FloatBarrier

\begin{table}[H]
\caption{
  \label{tab:perl-confirmed}
  Confirmed AHFA states for a Perl grammar}
\vspace{1ex}
\begin{tabular}{|r|r|}
\hline
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size}&
  \multicolumn{1}{|c|}{Percent}\\
& \multicolumn{1}{|c|}{of occurrences}\\
\hline
\rule{0pt}{2.3ex}
1&67.05\%\\
2&25.67\%\\
3&2.87\%\\
4&2.68\%\\
5&0.19\%\\
6&0.38\%\\
7&0.19\%\\
8&0.57\%\\
9&0.19\%\\
20&0.19\%\\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\subsubsection{Confirmed states for a Perl grammar}
We show the counts for the confirmed states for the Perl grammar
in Table \ref{tab:perl-confirmed}
on page \pageref{tab:perl-confirmed}.
They range in size from 1 to 20 items,
but the numbers are heavily skewed toward the low
end.
As can be seen, well over 90\% of the total confirmed states have
just one or two items.
The average size is 1.5235,
and the average of the size squared is 3.9405.
\FloatBarrier

\begin{table}[H]
\caption{
  \label{tab:html-confirmed}
  Confirmed AHFA states for HTML grammars}
\vspace{1ex}
\begin{tabular}{|r|r|}
\hline
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size}&
  \multicolumn{1}{|c|}{Percent}\\
& \multicolumn{1}{|c|}{of occurrences}\\
\hline
\rule{0pt}{2.3ex}
1&80.96\%\\
2&19.04\%\\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\subsubsection{Confirmed states for HTML grammars}
For HTML, I looked at a parser which generates grammars on
the fly, aggregating the states in all of them.
The data are in
Table \ref{tab:html-confirmed}
on page \pageref{tab:html-confirmed}.
The average size is 1.1904,
and the average of the size squared is 1.5712.

\FloatBarrier % Necessary to make 'Needspace' below work
%             % to prevent orphan
\begin{table}[H]
\caption{
  \label{tab:c-confirmed}
  Confirmed AHFA states for a C grammar}
\vspace{1ex}
\begin{tabular}{|r|r|}
\hline
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size}&
  \multicolumn{1}{|c|}{Occurrences}\\
\hline
\rule{0pt}{2.3ex}
1&695\\
2&188\\
3&40\\
4&17\\
5&6\\
6&8\\
7&6\\
8&4\\
9&1\\
10&2\\
12&2\\
15&1\\
\hline
\end{tabular}
\end{table}

\FloatBarrier
Table \ref{tab:c-confirmed}
on page \pageref{tab:c-confirmed}
shows the count of confirmed AHFA states, by state size,
for a compiler-quality C grammar,
the confirmed states range in size from 1 to 15 items but again,
the numbers are heavily skewed toward the low
end.  Here are the item counts that appear, with the percent of the total
confirmed AHFA states with that item count in parentheses.
There were 970 confirmed C states.
The average size was 1.52.
The average of the size squared was 3.98.
\FloatBarrier

\subsection{Predicted states.}

The number of predicted states tends to be much more
evenly distributed.
It also tends to be much larger, and
the average for practical grammars may be $O(s)$,
where $s$ is the size of the grammar.
This is the same as the theoretical worst case.
For predicted states, because AHFA states of varied
size are common,
we switch from showing the frequency of AHFA states
for each size,
to showing the sizes of the AHFA state by the
frequency of states of that size.

\FloatBarrier
\begin{table}[H]
\caption{
  \label{tab:perl-predicted}
  Predicted AHFA states for a Perl grammar}
\vspace{1ex}
\begin{tabular}{|l|r|}
\hline
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size of AHFA state}&
  \multicolumn{1}{|c|}{Occurrences of states}\\
   &\multicolumn{1}{|c|}{of that size}\\
\hline
\rule{0pt}{2.3ex}%
2&5\\
\hline
\rule{0pt}{2.3ex}%
3, 142&4\\
\hline
\rule{0pt}{2.3ex}%
1, 4&3\\
\hline
\rule{0pt}{2.3ex}%
6, 7, 143&2\\
\hline
\cellbox[3in]{
  5, 64, 71, 77, 79, 81, 83, 85, 88, 90, 98, 100, 102,
  104, 106, 108, 111, 116, 127, 129, 132, 135, 136, 137, 141,
  144, 149, 151, 156, 157, 220, 224, 225
}
  &1\\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\subsubsection{Predicted states for a Perl grammar}

Table \ref{tab:perl-predicted}
on page \pageref{tab:perl-predicted}
shows the data for a Perl grammar.
The number of predicted states in the Perl grammar was 58.
The average size was 83.59 AHFA items.
The average of the size squared was 11356.41.
\FloatBarrier

\begin{table}[H]
\caption{
  \label{tab:html-predicted}
  Predicted AHFA states for HTML grammars}
\vspace{1ex}
\begin{tabular}{|r|r|c|r|r|}
\cline{1-2} \cline{4-5}
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size}&
  \multicolumn{1}{|c|}{Occurrences}&
  \multicolumn{1}{|c|}{}&
\multicolumn{1}{|c|}{Size}&
  \multicolumn{1}{|c|}{Occurrences}\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
1&95&&
20&190\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
2&95&&
21&63\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
4&95&&
22&22\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
11&181&&
24&8\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
14&181&&
25&16\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
15&294&&
26&16\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
16&112&&
28&2\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
18&349&&
29&16\\
\cline{1-2} \cline{4-5}
\rule{0pt}{2.3ex}%
19&120\\
\cline{1-2}
\end{tabular}
\end{table}

\FloatBarrier
\subsubsection{Predicted states for HTML grammars}

Data for predicted AHFA states in the HTML grammars
is in Table \ref{tab:html-predicted}
on page \pageref{tab:html-predicted}.
The total number of predicted states in the HTML grammars was
1855. Their average size was 14.60. Their average size squared was
250.93.
\FloatBarrier

\begin{table}[H]
\caption{
  \label{tab:c-predicted}
  Predicted AHFA states for a C grammar}
\vspace{1ex}
\begin{tabular}{|l|r|}
\hline
\multicolumn{1}{|c|}
  {\rule{0pt}{2.3ex}
   \rule{0pt}{-.5ex}
   Size of AHFA state}&
  \multicolumn{1}{|c|}{Occurrences of states}\\
   &\multicolumn{1}{|c|}{of that size}\\
\hline
\rule{0pt}{2.3ex}%
2, 3&6\\
\hline
\rule{0pt}{2.3ex}%
8&5\\
\hline
\rule{0pt}{2.3ex}%
4, 90&4\\
\hline
\rule{0pt}{2.3ex}%
6, 11, 31, 47&3\\
\hline
\rule{0pt}{2.3ex}%
\cellbox[3in]{5, 14, 42, 64, 68, 78, 91, 95, 98}
&2\\
\hline
\cellbox[3in]{
1, 7, 9, 12, 15, 17, 18, 19, 21, 22, 25, 28, 29, 33, 34, 36,
37, 40, 43, 44, 45, 46, 52, 53, 54, 57, 58, 61, 65, 66, 69, 72,
74, 76, 80, 81, 86, 87, 89, 94, 96, 97, 99, 102, 105, 108,
115, 117, 119, 123, 125, 127, 144, 149, 150, 154, 181, 219,
222.
}
  &1\\
\hline
\end{tabular}
\end{table}

\FloatBarrier
\subsubsection{Predicted states for a C grammar}

The data for predicted states in a C grammar
is Table \ref{tab:c-predicted}
on page \pageref{tab:c-predicted}
The number of predicted states in the C grammar was 114.
The average size was 54.81.
The average size squared was 5361.28.
The sizes of the predicted states for the C grammar were spread from 1
to 222.
\FloatBarrier

\subsection{Completed LHS symbols per AHFA state. }
An AHFA state may contain completions for more than one LHS,
but that is rare in practical use, and the number of completed
LHS symbols in the exceptions remains low.
The very complex Perl AHFA contains 271 states with completions.
Of these 268 have only one completed symbol.
The other three AHFA states complete only two different LHS symbols.
Two states have completions with both
a \texttt{term\_hi} and an \texttt{indirob} on the LHS.
One state has completions for both a
\texttt{sideff} and an \texttt{mexpr}.

My HTML test grammars make the
same point more strongly.
My HTML parser generates grammars on the fly.
These HTML grammars can differ from each other.
because Marpa takes the HTML input into account when
generating the grammar.
In my HTML test suite,
every single one
of the 14,782 AHFA states
has only one completed LHS symbol.

\FloatBarrier

\clearpage
\tableofcontents

\end{document}
